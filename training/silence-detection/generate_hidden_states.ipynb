{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd7da45-888c-43ce-af7f-6a1e0c110012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_hidden_states_and_audio(text: str, speaker_id: str) -> tuple[np.ndarray, torch.Tensor, dict]:\n",
    "    \"\"\"\n",
    "    Get hidden states and audio for a piece of text using specified speaker\n",
    "    \n",
    "    Returns:\n",
    "        hidden_states: numpy array of shape [n_frames, hidden_dim]\n",
    "        audio: torch tensor of shape [1, samples] \n",
    "        metadata: dict with frame info\n",
    "    \"\"\"\n",
    "    response = requests.post(\n",
    "        \"http://melchior:5000/v1/audio/speech/hidden\",\n",
    "        json={\n",
    "            \"text\": text,\n",
    "            \"speaker_id\": speaker_id,\n",
    "            \"return_audio\": True\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Request failed with status {response.status_code}: {response.text}\")\n",
    "    \n",
    "    # Read zip contents in memory\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zf:\n",
    "        # Get hidden states\n",
    "        with zf.open('hidden_states.npy') as f:\n",
    "            hidden_states = np.load(io.BytesIO(f.read()))\n",
    "        \n",
    "        # Get audio\n",
    "        with zf.open('audio.wav') as f:\n",
    "            # Use torchaudio to load WAV directly from bytes\n",
    "            audio, sr = torchaudio.load(io.BytesIO(f.read()))\n",
    "            assert sr == 44100\n",
    "        \n",
    "        # Get metadata\n",
    "        with zf.open('metadata.json') as f:\n",
    "            metadata = json.loads(f.read())\n",
    "            \n",
    "    return hidden_states, audio, metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b152ce-b624-4121-806f-6d80f9660e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_silence(audio: torch.Tensor, frame_rate=21.535, threshold=0.005):\n",
    "    \"\"\"Dead simple RMS silence detection\"\"\"\n",
    "    sr = 44100\n",
    "    samples_per_frame = int(sr / frame_rate)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if audio.dim() > 1:\n",
    "        audio = audio[0]\n",
    "    \n",
    "    # Compute RMS energy per frame\n",
    "    windows = audio.unfold(0, samples_per_frame, samples_per_frame)\n",
    "    energy = torch.sqrt(torch.mean(windows ** 2, dim=1))\n",
    "    \n",
    "    return energy < threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b611dc-78be-450c-8db7-f87ebf008f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_buffer(lang_code, buffer_size=1000):\n",
    "    \"\"\"Get a buffer of samples for a language, reuse until empty\"\"\"\n",
    "    path = f\"{lang_code.upper()}/*.tar\"\n",
    "    dataset = load_dataset(\n",
    "        \"amphion/Emilia-Dataset\",\n",
    "        data_files={lang_code.lower(): path},\n",
    "        split=lang_code.lower(),\n",
    "        streaming=True\n",
    "    )\n",
    "    \n",
    "    buffer = []\n",
    "    for sample in dataset:\n",
    "        buffer.append(sample)\n",
    "        if len(buffer) >= buffer_size:\n",
    "            break\n",
    "            \n",
    "    return buffer\n",
    "\n",
    "def create_silence_dataset(threshold=0.005, n_train=500, n_test=250, n_val=100, seed=42, buffer_size=1000):\n",
    "    \"\"\"Create silence detection dataset balanced across languages\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Load and group speakers by language\n",
    "    with open('./voices/index.json') as f:\n",
    "        speakers = json.load(f)['speakers']\n",
    "    \n",
    "    lang_speakers = defaultdict(list)\n",
    "    for speaker_id in speakers:\n",
    "        if speaker_id != 'default':\n",
    "            lang = speaker_id.split('_')[0]\n",
    "            lang_speakers[lang].append(speaker_id)\n",
    "    \n",
    "    samples_per_lang = {\n",
    "        'EN': n_train + n_test + n_val,\n",
    "        'JA': n_train + n_test + n_val,\n",
    "        'ZH': n_train + n_test + n_val\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for lang, n_samples in tqdm(samples_per_lang.items(), desc=\"Languages\"):\n",
    "        # Get a buffer of samples for this language\n",
    "        buffer = get_language_buffer(lang.lower(), buffer_size)\n",
    "        \n",
    "        samples_collected = 0\n",
    "        with tqdm(total=n_samples, desc=f\"{lang} samples\") as pbar:\n",
    "            while samples_collected < n_samples:\n",
    "                if not buffer:  # If buffer empty, refill it\n",
    "                    buffer = get_language_buffer(lang.lower(), buffer_size)\n",
    "                \n",
    "                # Get a random sample from buffer\n",
    "                sample = random.choice(buffer)\n",
    "                buffer.remove(sample)  # Remove used sample\n",
    "                \n",
    "                # Get a random speaker for this language\n",
    "                speaker_id = random.choice(lang_speakers[lang])\n",
    "                \n",
    "                try:\n",
    "                    hidden, audio, meta = get_hidden_states_and_audio(\n",
    "                        sample['json']['text'],\n",
    "                        speaker_id\n",
    "                    )\n",
    "                    silence_mask = rms_silence(audio, threshold=threshold)\n",
    "                    \n",
    "                    all_data.append({\n",
    "                        'hidden_states': hidden,\n",
    "                        'is_silence': silence_mask.numpy(),\n",
    "                        'language': lang,\n",
    "                        'n_frames': len(silence_mask),\n",
    "                        'speaker_id': speaker_id,\n",
    "                        'text': sample['json']['text']\n",
    "                    })\n",
    "                    \n",
    "                    samples_collected += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sample: {e}\")\n",
    "                    continue\n",
    "    \n",
    "    # Split into train/test/val\n",
    "    random.shuffle(all_data)\n",
    "    splits = {\n",
    "        'train': all_data[:n_train*3],  # *3 because we have 3 languages\n",
    "        'test': all_data[n_train*3:n_train*3 + n_test*3],\n",
    "        'val': all_data[n_train*3 + n_test*3:]\n",
    "    }\n",
    "    \n",
    "    # Convert to HF datasets\n",
    "    output_path = Path('./silence_dataset_prod')\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    for split_name, split_data in splits.items():\n",
    "        # Convert to format HF datasets likes\n",
    "        ds_dict = {\n",
    "            'hidden_states': [d['hidden_states'] for d in split_data],\n",
    "            'is_silence': [d['is_silence'] for d in split_data],\n",
    "            'language': [d['language'] for d in split_data],\n",
    "            'n_frames': [d['n_frames'] for d in split_data],\n",
    "            'speaker_id': [d['speaker_id'] for d in split_data],\n",
    "            'text': [d['text'] for d in split_data]\n",
    "        }\n",
    "        \n",
    "        # Create and save dataset\n",
    "        ds = Dataset.from_dict(ds_dict)\n",
    "        ds.save_to_disk(output_path / split_name)\n",
    "        \n",
    "    # Save metadata\n",
    "    with open(output_path / 'metadata.json', 'w') as f:\n",
    "        json.dump({\n",
    "            'threshold': threshold,\n",
    "            'n_train': n_train,\n",
    "            'n_test': n_test,\n",
    "            'n_val': n_val,\n",
    "            'seed': seed\n",
    "        }, f)\n",
    "    \n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2124e0-c3f4-4f95-9520-3631f644f83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498185205fe9471bbec338fce746dbb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Languages:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe5b5234f804725aeffca7e3ee30b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6b4133748843e5949bb2140ad90d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1599a8a636db41d1ac0707b0b69b8b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "EN samples:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9031c337854a488579bb38aa7fa600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01850bcf2f2f46268619878d79efa445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48164827083f476597e257e695ca8e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JA samples:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936536f4b0be4cd08f57660d8c29f4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143505ce2f034a57a065227f6dc0fe34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92febf6bf62947d3bd45875a443eb5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ZH samples:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8454cb3e134d4720bada19e2e6f507b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd4e3da785a45638c000d9c6696d0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/150 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d080e4c2c4043d29ce996cb4b909405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to silence_dataset\n"
     ]
    }
   ],
   "source": [
    "dataset_path = create_silence_dataset(\n",
    "    threshold=0.005,\n",
    "    n_train=100,\n",
    "    n_test=50,\n",
    "    n_val=10,\n",
    "    seed=42,\n",
    "    buffer_size=1000\n",
    ")\n",
    "print(f\"Dataset saved to {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba218357-a640-4295-840b-9ff3c24aa0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class balance:\n",
      "Train: 10062/57054 silent frames (17.6%)\n",
      "Val: 1253/6347 silent frames (19.7%)\n",
      "\n",
      "Metrics at different thresholds:\n",
      "\n",
      "Threshold 0.3:\n",
      "Precision: 0.9332\n",
      "Recall: 0.9473\n",
      "F1: 0.9402\n",
      "False positives: 85\n",
      "False negatives: 66\n",
      "\n",
      "Threshold 0.4:\n",
      "Precision: 0.9387\n",
      "Recall: 0.9409\n",
      "F1: 0.9398\n",
      "False positives: 77\n",
      "False negatives: 74\n",
      "\n",
      "Threshold 0.5:\n",
      "Precision: 0.9436\n",
      "Recall: 0.9346\n",
      "F1: 0.9391\n",
      "False positives: 70\n",
      "False negatives: 82\n",
      "\n",
      "Threshold 0.6:\n",
      "Precision: 0.9472\n",
      "Recall: 0.9314\n",
      "F1: 0.9392\n",
      "False positives: 65\n",
      "False negatives: 86\n",
      "\n",
      "Threshold 0.7:\n",
      "Precision: 0.9507\n",
      "Recall: 0.9234\n",
      "F1: 0.9368\n",
      "False positives: 60\n",
      "False negatives: 96\n"
     ]
    }
   ],
   "source": [
    "def analyze_balance_and_metrics(dataset_path, model):\n",
    "    train_ds = load_from_disk(str(Path(dataset_path) / 'train'))\n",
    "    val_ds = load_from_disk(str(Path(dataset_path) / 'val'))\n",
    "    \n",
    "    # Get class balance\n",
    "    def count_labels(ds):\n",
    "        silence_counts = sum(np.sum(s) for s in ds['is_silence'])\n",
    "        total_frames = sum(len(s) for s in ds['is_silence'])\n",
    "        return silence_counts, total_frames\n",
    "    \n",
    "    train_silence, train_total = count_labels(train_ds)\n",
    "    val_silence, val_total = count_labels(val_ds)\n",
    "    \n",
    "    print(\"Class balance:\")\n",
    "    print(f\"Train: {train_silence}/{train_total} silent frames ({100*train_silence/train_total:.1f}%)\")\n",
    "    print(f\"Val: {val_silence}/{val_total} silent frames ({100*val_silence/val_total:.1f}%)\")\n",
    "    \n",
    "    # Detailed metrics on validation set\n",
    "    X_val = np.vstack([h for sample in val_ds['hidden_states'] for h in sample])\n",
    "    y_val = np.hstack([s for sample in val_ds['is_silence'] for s in sample])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_pred = model(torch.FloatTensor(X_val)).squeeze().numpy()\n",
    "    \n",
    "    # Check different thresholds\n",
    "    print(\"\\nMetrics at different thresholds:\")\n",
    "    for threshold in [0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "        pred_bool = val_pred > threshold\n",
    "        true_pos = (pred_bool & y_val.astype(bool)).sum()\n",
    "        false_pos = (pred_bool & ~y_val.astype(bool)).sum()\n",
    "        false_neg = (~pred_bool & y_val.astype(bool)).sum()\n",
    "        \n",
    "        precision = true_pos / (true_pos + false_pos)\n",
    "        recall = true_pos / (true_pos + false_neg)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        print(f\"\\nThreshold {threshold}:\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1: {f1:.4f}\")\n",
    "        print(f\"False positives: {false_pos}\")\n",
    "        print(f\"False negatives: {false_neg}\")\n",
    "\n",
    "# Check it\n",
    "analyze_balance_and_metrics('./silence_dataset', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "566b66b1-7f6d-42bd-9ff5-adad97d02a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 57054 frames from 300 samples\n",
      "Validating on 6347 frames from 30 samples\n",
      "Epoch 1/20\n",
      "Train loss: 0.7218, Train acc: 0.8280\n",
      "Val loss: 0.5939, Val acc: 0.9570\n",
      "\n",
      "Epoch 2/20\n",
      "Train loss: 0.5818, Train acc: 0.9689\n",
      "Val loss: 0.5883, Val acc: 0.9627\n",
      "\n",
      "Epoch 3/20\n",
      "Train loss: 0.5680, Train acc: 0.9832\n",
      "Val loss: 0.5806, Val acc: 0.9701\n",
      "\n",
      "Epoch 4/20\n",
      "Train loss: 0.5654, Train acc: 0.9858\n",
      "Val loss: 0.5723, Val acc: 0.9786\n",
      "\n",
      "Epoch 5/20\n",
      "Train loss: 0.5611, Train acc: 0.9902\n",
      "Val loss: 0.5788, Val acc: 0.9720\n",
      "\n",
      "Normalized Confusion Matrix:\n",
      "    EN  JA  ZH\n",
      "EN: [0.9725330471992493, 0.016954900696873665, 0.010512038134038448]\n",
      "JA: [0.03423607721924782, 0.9519672989845276, 0.013796627521514893]\n",
      "ZH: [0.0, 0.002081887563690543, 0.9979181289672852]\n",
      "\n",
      "Epoch 6/20\n",
      "Train loss: 0.5602, Train acc: 0.9909\n",
      "Val loss: 0.5782, Val acc: 0.9726\n",
      "\n",
      "Epoch 7/20\n",
      "Train loss: 0.5574, Train acc: 0.9940\n",
      "Val loss: 0.5784, Val acc: 0.9726\n",
      "\n",
      "Epoch 8/20\n",
      "Train loss: 0.5570, Train acc: 0.9943\n",
      "Val loss: 0.5659, Val acc: 0.9849\n",
      "\n",
      "Epoch 9/20\n",
      "Train loss: 0.5613, Train acc: 0.9900\n",
      "Val loss: 0.5683, Val acc: 0.9830\n",
      "\n",
      "Epoch 10/20\n",
      "Train loss: 0.5582, Train acc: 0.9931\n",
      "Val loss: 0.5784, Val acc: 0.9724\n",
      "\n",
      "Normalized Confusion Matrix:\n",
      "    EN  JA  ZH\n",
      "EN: [0.9854187965393066, 0.00983384158462286, 0.00474737212061882]\n",
      "JA: [0.05365355312824249, 0.9381706714630127, 0.00817577913403511]\n",
      "ZH: [0.0013879250036552548, 0.006245662923902273, 0.9923664331436157]\n",
      "\n",
      "Epoch 11/20\n",
      "Train loss: 0.5566, Train acc: 0.9947\n",
      "Val loss: 0.5726, Val acc: 0.9787\n",
      "\n",
      "Epoch 12/20\n",
      "Train loss: 0.5575, Train acc: 0.9939\n",
      "Val loss: 0.5736, Val acc: 0.9776\n",
      "\n",
      "Epoch 13/20\n",
      "Train loss: 0.5568, Train acc: 0.9946\n",
      "Val loss: 0.5744, Val acc: 0.9764\n",
      "\n",
      "Epoch 14/20\n",
      "Train loss: 0.5573, Train acc: 0.9939\n",
      "Val loss: 0.5740, Val acc: 0.9772\n",
      "\n",
      "Epoch 15/20\n",
      "Train loss: 0.5583, Train acc: 0.9930\n",
      "Val loss: 0.5741, Val acc: 0.9770\n",
      "\n",
      "Normalized Confusion Matrix:\n",
      "    EN  JA  ZH\n",
      "EN: [0.9877924919128418, 0.011190233752131462, 0.0010172940092161298]\n",
      "JA: [0.029126213863492012, 0.9621869921684265, 0.008686765097081661]\n",
      "ZH: [0.006245662923902273, 0.018736988306045532, 0.9750173687934875]\n",
      "\n",
      "Epoch 16/20\n",
      "Train loss: 0.5575, Train acc: 0.9938\n",
      "Val loss: 0.5706, Val acc: 0.9808\n",
      "\n",
      "Epoch 17/20\n",
      "Train loss: 0.5583, Train acc: 0.9930\n",
      "Val loss: 0.5819, Val acc: 0.9690\n",
      "\n",
      "Epoch 18/20\n",
      "Train loss: 0.5578, Train acc: 0.9935\n",
      "Val loss: 0.5728, Val acc: 0.9783\n",
      "\n",
      "Epoch 19/20\n",
      "Train loss: 0.5557, Train acc: 0.9956\n",
      "Val loss: 0.5638, Val acc: 0.9872\n",
      "\n",
      "Epoch 20/20\n",
      "Train loss: 0.5561, Train acc: 0.9952\n",
      "Val loss: 0.5649, Val acc: 0.9861\n",
      "\n",
      "Normalized Confusion Matrix:\n",
      "    EN  JA  ZH\n",
      "EN: [0.9877924919128418, 0.011529332026839256, 0.0006781960255466402]\n",
      "JA: [0.01635155826807022, 0.9826264977455139, 0.0010219723917543888]\n",
      "ZH: [0.0013879250036552548, 0.011103400029242039, 0.9875086545944214]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_language_probe(dataset_path, batch_size=256, lr=1e-3, epochs=20, device=\"cuda\"):\n",
    "    train_ds = load_from_disk(str(Path(dataset_path) / 'train'))\n",
    "    val_ds = load_from_disk(str(Path(dataset_path) / 'val'))\n",
    "    \n",
    "    # Create language mapping\n",
    "    lang_to_idx = {'EN': 0, 'JA': 1, 'ZH': 2}\n",
    "    \n",
    "    def prepare_data(ds):\n",
    "        hidden_states = []\n",
    "        languages = []\n",
    "        for i, sample in enumerate(ds):\n",
    "            # Keep all frames\n",
    "            hidden_states.append(sample['hidden_states'])\n",
    "            # Repeat language label for each frame\n",
    "            languages.extend([lang_to_idx[sample['language']]] * len(sample['hidden_states']))\n",
    "        \n",
    "        hidden_states = np.vstack(hidden_states)  # Stack all frames\n",
    "        return (torch.FloatTensor(hidden_states).to(device), \n",
    "                torch.LongTensor(languages).to(device))\n",
    "    \n",
    "    X_train, y_train = prepare_data(train_ds)\n",
    "    X_val, y_val = prepare_data(val_ds)\n",
    "    \n",
    "    print(f\"Training on {len(X_train)} frames from {len(train_ds)} samples\")\n",
    "    print(f\"Validating on {len(X_val)} frames from {len(val_ds)} samples\")\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(X_train, y_train), \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    model = LanguageProbe().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(batch_X)\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_correct += (pred.argmax(1) == batch_y).sum().item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        train_acc = train_correct / len(X_train)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val)\n",
    "            val_correct = (val_pred.argmax(1) == y_val).sum().item()\n",
    "            val_acc = val_correct / len(X_val)\n",
    "            \n",
    "            # Frame-level confusion matrix\n",
    "            conf_matrix = torch.zeros(3, 3)\n",
    "            preds = val_pred.argmax(1)\n",
    "            for t, p in zip(y_val, preds):\n",
    "                conf_matrix[t, p] += 1\n",
    "            # Normalize by true class counts\n",
    "            conf_matrix = conf_matrix / conf_matrix.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}\")\n",
    "        print(f\"Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(\"\\nNormalized Confusion Matrix:\")\n",
    "            print(\"    EN  JA  ZH\")\n",
    "            for i, lang in enumerate(['EN', 'JA', 'ZH']):\n",
    "                print(f\"{lang}: {conf_matrix[i].tolist()}\")\n",
    "        print()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Let's see how it does frame-by-frame!\n",
    "language_model = train_language_probe('./silence_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
